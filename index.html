<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="3D Semantic Instance Segmentation with Open-Vocabulary queries">
  <meta name="keywords" content="OpenMask3D, Instance Segmentation, 3D Scene Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenMask3D ðŸ›‹</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OP3Det<br />
            <p class="title is-3 publication-title">Towards 3D Objectness Learning in an Open World</p>
          </h1>          

          <h1 class="title is-4" style="color: #5c5c5c;">NeurIPS 2025</h1>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
              <a href="https://aycatakmaz.github.io/">Ay&#231;a Takmaz</a><sup>1*</sup>,</span> -->
            <span class="author-block">
              <a href="https://taichiliu.github.io/">Taichi Liu</a><sup>1</sup>,
            </span>
            <!-- <br /> -->
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=x_-kOjoAAAAJ&hl=zh-CN">Zhenyu Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://liux4189.github.io/">Ruofeng Liu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://guangwang.me/#/home">Guang Wang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.cs.rutgers.edu/~dz220/">Desheng Zhang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 1em;"><sup>1</sup>Rutgers University</span>
            <span class="author-block" style="margin-right: 1em;"><sup>2</sup>Tsinghua University</span>
            <span class="author-block" style="margin-right: 1em;"><sup>3</sup>Michigan State University</span>
            <span class="author-block" style="margin-right: 1em;"><sup>4</sup>Florida State University</span>
            <!-- <span class="author-block" style="margin-right: 1em; font-size: 18px;"><sup>*</sup>Equal contribution</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
    
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/example.png" class="center"/>
      <br><br>
      <h2 class="subtitle has-text-centered" style="white-space: nowrap;">
      <strong>OP3Det</strong> a class-agnostic open-world prompt-free 3D detector to detect any objects within 3D scenes
    </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in 3D object detection and novel category detection have made significant progress, 
            yet research on learning generalized 3D objectness remains insufficient. In this paper, 
            we delve into learning open-world 3D objectness, which focuses on detecting <i>all</i> objects in a 3D scene, 
            including novel objects unseen during training. Traditional closed-set 3D detectors struggle to generalize to open-world scenarios,
            while directly incorporating 3D open-vocabulary models for open-world ability struggles with vocabulary expansion and semantic overlap. 
            To achieve generalized 3D object discovery, we propose <strong>OP3Det</strong>, a class-agnostic <strong>O</strong>pen-World <strong>P</strong>rompt-free <strong>3</strong>D <strong>Det</strong>ector
            to detect any objects within 3D scenes without relying on hand-crafted text prompts.
            We introduce the strong generalization and zero-shot capabilities of 2D foundation models, utilizing both 2D semantic
            priors and 3D geometric priors for class-agnostic proposals to broaden 3D object
            discovery. Then, by integrating complementary information from point cloud
            and RGB image in the cross-modal mixture of experts, <strong>OP3Det</strong> dynamically
            routes uni-modal and multi-modal features to learn generalized 3D objectness.
            Extensive experiments demonstrate the extraordinary performance of <strong>OP3Det</strong>,
            which significantly surpasses existing open-world 3D detectors by up to 16.0% in
            AR and achieves a 13.5% improvement compared to closed-world 3D detectors.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Explanatory Video</h2>
        <video controls poster="static/images/explanatory_video_poster.jpeg">
          <source src="static/videos/poster_session.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div> -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
            We would like to thank Utkarsh Sinha and Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>


